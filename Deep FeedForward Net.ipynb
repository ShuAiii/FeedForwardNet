{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neuron Class Object ##\n",
    "class Neuron:\n",
    "    def __init__(self, weight, bias):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        \n",
    "    def feedforward(self, inputs, act_type):\n",
    "        total = np.dot(self.weight, inputs) + self.bias\n",
    "        if act_type == 0:\n",
    "            return sigmoid(total)\n",
    "        elif act_type == 1:\n",
    "            return softmax(total)\n",
    "        elif act_type == 2:\n",
    "            return relu(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neural Network Class Object ##\n",
    "class Neural_Net:\n",
    "    def __init__(self, n_num, weight, bias):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        self.n_num = n_num\n",
    "        cur_layer = []\n",
    "        self.all_layer = []\n",
    "        for j in range(len(n_num)-1):\n",
    "            for i in range(n_num[j+1]):\n",
    "                cur_layer.append(Neuron(weight[j][i], bias[j][i]))\n",
    "            self.all_layer.append(cur_layer)\n",
    "            cur_layer = []\n",
    "            \n",
    "    def feedforward(self, i_data):\n",
    "        cur_output = []\n",
    "        all_output = []\n",
    "        all_output.append(i_data.tolist())\n",
    "        for j in range(len(self.n_num)-1):\n",
    "            for i in range(self.n_num[j+1]):\n",
    "                x = self.all_layer[j][i].feedforward(all_output[j],0)\n",
    "                cur_output.append(x)\n",
    "            if type(cur_output)==list:\n",
    "                all_output.append(cur_output)\n",
    "            else:\n",
    "                all_output.append(cur_output.tolist())\n",
    "            cur_output = []\n",
    "        return all_output\n",
    "    \n",
    "    def backpropagation(self, pred, i_y):\n",
    "        dim = len(pred)\n",
    "        deriv_last = deriv_loss(i_y, pred[dim-1])\n",
    "        g_field=[]\n",
    "        for j in range(1, dim-1):\n",
    "            deriv_input = deriv_hidden(pred[j-1], self.weight[j-1], self.bias[j-1])\n",
    "            deriv = deriv_input\n",
    "            for i in range(j, dim-1):\n",
    "                deriv = np.matmul(deriv_pred(pred[i], self.weight[i], self.bias[i]), deriv)\n",
    "            deriv = np.matmul(deriv_last, deriv)\n",
    "            g_field.append(deriv)\n",
    "        g_field_last = np.matmul(deriv_last, deriv_hidden(pred[dim-2], self.weight[dim-2], self.bias[dim-2]))\n",
    "        g_field.append(g_field_last)\n",
    "        w_field = []\n",
    "        b_field = []\n",
    "        for i in range(len(g_field)):\n",
    "            w_field.append(np.reshape(g_field[i][:len(g_field[i])-len(self.weight[i])], (self.n_num[i+1],self.n_num[i])).tolist())\n",
    "            b_field.append(g_field[i][len(g_field[i])-len(self.weight[i]):].tolist())\n",
    "        return w_field, b_field\n",
    "    \n",
    "    def gradient_descent(self, w_field, b_field, learning_rate):\n",
    "        for i in range(0, 2):\n",
    "            self.weight[i] = np.subtract(self.weight[i],np.multiply(learning_rate, w_field[i])).tolist()\n",
    "            self.bias[i] = np.subtract(self.bias[i], np.multiply(learning_rate, b_field[i])).tolist()\n",
    "        return self.weight, self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HELPER FUNCTIONS ##\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    fx = sigmoid(x)\n",
    "    return fx * (1 - fx)\n",
    "\n",
    "def nth_deriv_sigmoid(x, n):\n",
    "    coeff = np.zeros(n + 1, dtype=int)\n",
    "    coeff[0] = 1\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(i, -1, -1):\n",
    "            coeff[j] = -j * coeff[j - 1] + (j + 1) * coeff[j]\n",
    "    deriv = 0.0\n",
    "    for i in range(n, -1, -1):\n",
    "        deriv = sigmoid(x) * (coeff[i] + res)\n",
    "    return deriv\n",
    "\n",
    "def deriv_relu(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def deriv_loss(y_true, y_pred):\n",
    "    return -2*(y_true - y_pred)\n",
    "\n",
    "def deriv_pred(hidden, o_weight, o_bias):\n",
    "    return o_weight * deriv_sigmoid(np.dot(o_weight, hidden) + o_bias)[:,None]\n",
    "\n",
    "def deriv_hidden(i_data, weight, bias):\n",
    "    weight = np.array(weight)\n",
    "    n_dim = weight.shape[0]\n",
    "    w_dim = weight.shape[1]\n",
    "    derv = np.zeros((n_dim, n_dim * w_dim + n_dim))\n",
    "    for i in range(0,n_dim):\n",
    "        derv[i : i+1, i*w_dim : (i+1)*w_dim] = np.array(i_data) * deriv_sigmoid(np.dot(weight[i,:],i_data) + bias[i])\n",
    "        derv[i : i+1, n_dim*w_dim+i : n_dim*w_dim+i+1] = deriv_sigmoid(np.dot(weight[i,:],i_data) + bias[i])\n",
    "    return derv\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-48  -3   2   1]\n",
      " [-46 -20   3   1]\n",
      " [ 12  20   4   1]\n",
      " ...\n",
      " [ 49   7   5   1]\n",
      " [-40 -13   3   1]\n",
      " [-38  16   2   0]]\n"
     ]
    }
   ],
   "source": [
    "## Input Data ##\n",
    "import csv\n",
    "with open('Gender.csv') as csvfile: ## Weight, Height, Body type Data from Kaggle \n",
    "## https://www.kaggle.com/yersever/500-person-gender-height-weight-bodymassindex ##\n",
    "\n",
    "    #readCSV = csv.reader(csvfile, delimiter=',',quoting=csv.QUOTE_NONNUMERIC)\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    sex = []\n",
    "    weight = []\n",
    "    height = []\n",
    "    index = []\n",
    "    for row in readCSV:\n",
    "        sex.append(row[0])\n",
    "        height.append(row[1])\n",
    "        weight.append(row[2])\n",
    "        index.append(row[3])\n",
    "        \n",
    "sex = np.array(sex[1:])\n",
    "index = np.array(index[1:]).astype(int)\n",
    "#index = np.subtract(rindex,round(np.mean(rindex)))\n",
    "rheight = np.array(height[1:]).astype(int)\n",
    "height = np.subtract(rheight,round(np.mean(rheight)))\n",
    "rweight = np.array(weight[1:]).astype(int)\n",
    "weight = np.subtract(rweight,round(np.mean(rweight)))\n",
    "sex = np.where(sex=='Male',0,1)\n",
    "i_data = np.transpose(np.array([weight, height, index, sex])).astype(int)\n",
    "#i_data = sorted(i_data, key=lambda x : x[0])\n",
    "#print(i_data)\n",
    "#bot_10 = round(0.10 * len(i_data))\n",
    "#top_10 = round(0.90 * len(i_data))\n",
    "#i_data = i_data[:top_10]\n",
    "#i_data = i_data[bot_10:]\n",
    "#i_data = sorted(i_data, key=lambda x : x[1])\n",
    "#bot_10 = round(0.05 * len(i_data))\n",
    "#top_10 = round(0.95 * len(i_data))\n",
    "#i_data = i_data[:top_10]\n",
    "#i_data = i_data[bot_10:]\n",
    "np.random.shuffle(i_data)\n",
    "print(i_data)\n",
    "#for i in range(len(i_data)):\n",
    "#    i_data[i] = i_data[i].tolist()\n",
    "#batch = np.array_split(i_data,6);\n",
    "#print(batch)\n",
    "#print(batch[0].shape)\n",
    "#i_data = np.array([[-2, -1, 1],[25, 6, 0],[17, 4, 0],[-15, -6, 1]])\n",
    "#i_data = np.array([[-2, -1,1]])\n",
    "#print(i_data.shape)\n",
    "#n = i_data.shape[1]\n",
    "#i_x = i_data[: , 0:n-1]\n",
    "#i_y = i_data[: , n-1:n]\n",
    "#print(i_x)\n",
    "#print(i_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RANDOMIZE WEIGHTS ##\n",
    "def set_hyper(n_num, x_dim):\n",
    "    n_num.insert(0, x_dim)\n",
    "    weight = []\n",
    "    bias = []\n",
    "    for i in range(len(n_num)-1):\n",
    "        weight.append(np.transpose(np.random.normal(size=(n_num[i], n_num[i+1]))).tolist())\n",
    "        bias.append(np.random.normal(size=(n_num[i+1])).tolist())\n",
    "    return n_num, weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function ##\n",
    "def train(i_data, n_vec, learning_rate, epoch):\n",
    "    n = i_data.shape[1]\n",
    "    obs_n = i_data.shape[0]\n",
    "    i_y = i_data[: , n-1:n]\n",
    "    i_x = i_data[: , 0:n-1]\n",
    "    \n",
    "    n_num, weight, bias = set_hyper(n_vec, n-1)\n",
    "    \n",
    "    #weight = [[[0.5,0.4],[0.3,0.2]],[[0.4,0.3],[0.2,0.1]],[[0.6,0.7]]]\n",
    "    #bias = [[0.5,0.4],[0.4,0.3],[0.3]]\n",
    "    #print(weight)\n",
    "    #print(bias)\n",
    "    \n",
    "    for j in range(epoch):\n",
    "        loss = 0\n",
    "        #np.random.shuffle(i_data)\n",
    "        i_y = i_data[:, n-1:n]\n",
    "        i_x = i_data[:, 0:n-1]\n",
    "        for i in range(0,obs_n):\n",
    "            #print(\"\\n\")\n",
    "            #print(\"Loop Number:\",i)\n",
    "            network = Neural_Net(n_num, weight, bias)\n",
    "            y_pred = network.feedforward(i_x[i, :])\n",
    "            w_field,b_field = network.backpropagation(y_pred, i_y[i])\n",
    "            weight, bias = network.gradient_descent(w_field, b_field, learning_rate)\n",
    "            #print(\"True Y\", i_y[i])\n",
    "            #print(\"W_field\", w_field)\n",
    "            #print(\"B_field\", b_field)\n",
    "            #print(\"New Weight\", weight)\n",
    "            #print(\"New Bias\", bias)\n",
    "            #print(\"Predicted Y\", y_pred)\n",
    "            loss = loss + mse_loss(i_y[i], y_pred[2])\n",
    "            #print(\"Loss:\",loss)\n",
    "        loss = loss / i_data.shape[0]\n",
    "        print(\"Epoch %d loss: %.3f\" % (j + 1, loss))\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.427\n",
      "Epoch 2 loss: 0.420\n",
      "Epoch 3 loss: 0.414\n",
      "Epoch 4 loss: 0.407\n",
      "Epoch 5 loss: 0.402\n",
      "Epoch 6 loss: 0.398\n",
      "Epoch 7 loss: 0.397\n",
      "Epoch 8 loss: 0.397\n",
      "Epoch 9 loss: 0.398\n",
      "Epoch 10 loss: 0.399\n",
      "Epoch 11 loss: 0.401\n",
      "Epoch 12 loss: 0.402\n",
      "Epoch 13 loss: 0.403\n",
      "Epoch 14 loss: 0.404\n",
      "Epoch 15 loss: 0.405\n",
      "Epoch 16 loss: 0.406\n",
      "Epoch 17 loss: 0.406\n",
      "Epoch 18 loss: 0.406\n",
      "Epoch 19 loss: 0.406\n",
      "Epoch 20 loss: 0.406\n",
      "Epoch 21 loss: 0.406\n",
      "Epoch 22 loss: 0.406\n",
      "Epoch 23 loss: 0.405\n",
      "Epoch 24 loss: 0.405\n",
      "Epoch 25 loss: 0.404\n",
      "Epoch 26 loss: 0.404\n",
      "Epoch 27 loss: 0.403\n",
      "Epoch 28 loss: 0.402\n",
      "Epoch 29 loss: 0.401\n",
      "Epoch 30 loss: 0.399\n",
      "Epoch 31 loss: 0.398\n",
      "Epoch 32 loss: 0.395\n",
      "Epoch 33 loss: 0.393\n",
      "Epoch 34 loss: 0.390\n",
      "Epoch 35 loss: 0.387\n",
      "Epoch 36 loss: 0.383\n",
      "Epoch 37 loss: 0.380\n",
      "Epoch 38 loss: 0.377\n",
      "Epoch 39 loss: 0.374\n",
      "Epoch 40 loss: 0.371\n",
      "Epoch 41 loss: 0.368\n",
      "Epoch 42 loss: 0.365\n",
      "Epoch 43 loss: 0.362\n",
      "Epoch 44 loss: 0.360\n",
      "Epoch 45 loss: 0.359\n",
      "Epoch 46 loss: 0.359\n",
      "Epoch 47 loss: 0.361\n",
      "Epoch 48 loss: 0.362\n",
      "Epoch 49 loss: 0.364\n",
      "Epoch 50 loss: 0.365\n",
      "Epoch 51 loss: 0.366\n",
      "Epoch 52 loss: 0.367\n",
      "Epoch 53 loss: 0.367\n",
      "Epoch 54 loss: 0.366\n",
      "Epoch 55 loss: 0.365\n",
      "Epoch 56 loss: 0.364\n",
      "Epoch 57 loss: 0.363\n",
      "Epoch 58 loss: 0.362\n",
      "Epoch 59 loss: 0.360\n",
      "Epoch 60 loss: 0.359\n",
      "Epoch 61 loss: 0.357\n",
      "Epoch 62 loss: 0.356\n",
      "Epoch 63 loss: 0.354\n",
      "Epoch 64 loss: 0.353\n",
      "Epoch 65 loss: 0.351\n",
      "Epoch 66 loss: 0.350\n",
      "Epoch 67 loss: 0.348\n",
      "Epoch 68 loss: 0.347\n",
      "Epoch 69 loss: 0.346\n",
      "Epoch 70 loss: 0.345\n",
      "Epoch 71 loss: 0.344\n",
      "Epoch 72 loss: 0.343\n",
      "Epoch 73 loss: 0.342\n",
      "Epoch 74 loss: 0.341\n",
      "Epoch 75 loss: 0.340\n",
      "Epoch 76 loss: 0.339\n",
      "Epoch 77 loss: 0.339\n",
      "Epoch 78 loss: 0.338\n",
      "Epoch 79 loss: 0.337\n",
      "Epoch 80 loss: 0.336\n",
      "Epoch 81 loss: 0.336\n",
      "Epoch 82 loss: 0.335\n",
      "Epoch 83 loss: 0.335\n",
      "Epoch 84 loss: 0.334\n",
      "Epoch 85 loss: 0.333\n",
      "Epoch 86 loss: 0.333\n",
      "Epoch 87 loss: 0.332\n",
      "Epoch 88 loss: 0.332\n",
      "Epoch 89 loss: 0.332\n",
      "Epoch 90 loss: 0.331\n",
      "Epoch 91 loss: 0.331\n",
      "Epoch 92 loss: 0.330\n",
      "Epoch 93 loss: 0.330\n",
      "Epoch 94 loss: 0.330\n",
      "Epoch 95 loss: 0.329\n",
      "Epoch 96 loss: 0.329\n",
      "Epoch 97 loss: 0.329\n",
      "Epoch 98 loss: 0.328\n",
      "Epoch 99 loss: 0.328\n",
      "Epoch 100 loss: 0.328\n",
      "Epoch 101 loss: 0.328\n",
      "Epoch 102 loss: 0.327\n",
      "Epoch 103 loss: 0.327\n",
      "Epoch 104 loss: 0.327\n",
      "Epoch 105 loss: 0.327\n",
      "Epoch 106 loss: 0.326\n",
      "Epoch 107 loss: 0.326\n",
      "Epoch 108 loss: 0.326\n",
      "Epoch 109 loss: 0.326\n",
      "Epoch 110 loss: 0.326\n",
      "Epoch 111 loss: 0.325\n",
      "Epoch 112 loss: 0.325\n",
      "Epoch 113 loss: 0.325\n",
      "Epoch 114 loss: 0.325\n",
      "Epoch 115 loss: 0.325\n",
      "Epoch 116 loss: 0.325\n",
      "Epoch 117 loss: 0.324\n",
      "Epoch 118 loss: 0.324\n",
      "Epoch 119 loss: 0.324\n",
      "Epoch 120 loss: 0.324\n",
      "Epoch 121 loss: 0.324\n",
      "Epoch 122 loss: 0.324\n",
      "Epoch 123 loss: 0.323\n",
      "Epoch 124 loss: 0.323\n",
      "Epoch 125 loss: 0.323\n",
      "Epoch 126 loss: 0.323\n",
      "Epoch 127 loss: 0.323\n",
      "Epoch 128 loss: 0.323\n",
      "Epoch 129 loss: 0.323\n",
      "Epoch 130 loss: 0.322\n",
      "Epoch 131 loss: 0.322\n",
      "Epoch 132 loss: 0.322\n",
      "Epoch 133 loss: 0.322\n",
      "Epoch 134 loss: 0.322\n",
      "Epoch 135 loss: 0.322\n",
      "Epoch 136 loss: 0.322\n",
      "Epoch 137 loss: 0.322\n",
      "Epoch 138 loss: 0.321\n",
      "Epoch 139 loss: 0.321\n",
      "Epoch 140 loss: 0.321\n",
      "Epoch 141 loss: 0.321\n",
      "Epoch 142 loss: 0.321\n",
      "Epoch 143 loss: 0.321\n",
      "Epoch 144 loss: 0.321\n",
      "Epoch 145 loss: 0.320\n",
      "Epoch 146 loss: 0.320\n",
      "Epoch 147 loss: 0.320\n",
      "Epoch 148 loss: 0.320\n",
      "Epoch 149 loss: 0.320\n",
      "Epoch 150 loss: 0.320\n",
      "Epoch 151 loss: 0.320\n",
      "Epoch 152 loss: 0.320\n",
      "Epoch 153 loss: 0.319\n",
      "Epoch 154 loss: 0.319\n",
      "Epoch 155 loss: 0.319\n",
      "Epoch 156 loss: 0.319\n",
      "Epoch 157 loss: 0.319\n",
      "Epoch 158 loss: 0.319\n",
      "Epoch 159 loss: 0.318\n",
      "Epoch 160 loss: 0.318\n",
      "Epoch 161 loss: 0.318\n",
      "Epoch 162 loss: 0.318\n",
      "Epoch 163 loss: 0.318\n"
     ]
    }
   ],
   "source": [
    "network = train(i_data, [100,1,1], 0.001, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height Average(CM) 170.0\n",
      "Weight Average(KG) 106.0\n",
      "Body Type Index Average 4.0\n"
     ]
    }
   ],
   "source": [
    "## Average Metrics ##\n",
    "print(\"Height Average(CM)\", round(np.mean(rheight)))\n",
    "print(\"Weight Average(KG)\", round(np.mean(rweight)))\n",
    "print(\"Body Type Index Average\", round(np.mean(rindex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kevin: %.3f [0.42213294983886634]\n",
      "Alina: %.3f [0.5177929978865558]\n"
     ]
    }
   ],
   "source": [
    "## Prediction ##\n",
    "## [Weight, Height, Body Type] ##\n",
    "Boy = np.array([-26,20,3]) # Kevin 180 pounds, 74 inches\n",
    "Girl = np.array([-42, 11,0])  # Alina 141 pounds, 71 inches\n",
    "p1 = network.feedforward(Boy)\n",
    "p2 = network.feedforward(Girl)\n",
    "print(\"Kevin: %.3f\",  p1[2])\n",
    "print(\"Alina: %.3f\", p2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
